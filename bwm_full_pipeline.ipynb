{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Generating a Likelihood Table for a given pulsar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below walks through a full likelihood-table generation, diagnosis, and single-pulsar run using the likelihood table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import os, glob, json\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import scipy.linalg as sl\n",
    "import healpy as hp\n",
    "import multiprocessing as mp\n",
    "\n",
    "from enterprise.signals import parameter\n",
    "from enterprise.signals import signal_base\n",
    "from enterprise.signals import deterministic_signals\n",
    "from enterprise.signals import utils\n",
    "from enterprise import constants as const\n",
    "from enterprise.signals.signal_base import LogLikelihood\n",
    "from enterprise.signals.signal_base import LookupLikelihood\n",
    "\n",
    "from enterprise_extensions import models as ee_models\n",
    "from enterprise_extensions import model_utils as ee_model_utils\n",
    "from enterprise_extensions import sampler as ee_sampler\n",
    "\n",
    "from la_forge.core import Core, load_Core\n",
    "from la_forge import rednoise\n",
    "from la_forge.diagnostics import plot_chains\n",
    "\n",
    "from PTMCMCSampler.PTMCMCSampler import PTSampler as ptmcmc\n",
    "\n",
    "def make_lookup_table(psr, noisefile, outdir, sign, log10_amps, log10_amp_spacing, gammas, gamma_spacing, Ts, time_spacing):\n",
    "\n",
    "    if not os.path.exists(outdir + psr.name):\n",
    "        os.mkdir(outdir + psr.name)\n",
    "        \n",
    "    #now we need to make a pta for this pulsar to look up likelihoods for each amplitude we calculate\n",
    "    #################\n",
    "    ####   PTA   ####\n",
    "    #################\n",
    "\n",
    "\n",
    "    pta = ee_models.model_ramp([psr], LogLikelihood,\n",
    "                          upper_limit=False, bayesephem=False,\n",
    "                          Tmin_bwm=t0min, Tmax_bwm=t0max, logmin=min(log10_amps), logmax=max(log10_amps))\n",
    "    \n",
    "    print(\"Here are the parameters of the pta: {}\".format(pta.params))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with open(noisefile, 'rb') as nfile:\n",
    "       setpars = json.load(nfile)\n",
    "\n",
    "    pta.set_default_params(setpars)\n",
    "\n",
    "    with open(outdir + \"{}/{}_{}.txt\".format(psr.name, psr.name, sign),'a+') as f:\n",
    "        for t0 in Ts:\n",
    "            for log10_strain in log10_amps:\n",
    "                #set up the sky location of the pulsar\n",
    "                #ramp_amp*=sign\n",
    "                #print(psr.name + \"would see an amplitude of: \" + str(ramp_amp))\n",
    "\n",
    "                #Now we need to add the A_red and gamma_red params so that we have in total:\n",
    "                #A_red, Gamma_red, A_ramp, t_ramp\n",
    "                for log10_rn_amp in log10_amps:\n",
    "                    for gamma in gammas:\n",
    "                        #now we have the four parameters, we need to ask the pta to calculate a likelihood\n",
    "                        #the pta params are in the order:\n",
    "                        #[sign, psr_gamma, psr_log10_A, ramp_log10_A, ramp_t0]\n",
    "                        lnlike = pta.get_lnlikelihood([gamma, log10_rn_amp, log10_strain, t0, sign])\n",
    "                        f.write('{:.12f}\\n'.format(float(lnlike)).rjust(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pulsar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pkl_path = '/home/nima/nanograv/11yr_burst_factorizedlikelihood/NANOGrav_11yr_DE436.pickle'\n",
    "lookup_outdir = '/home/nima/nanograv/11yr_burst_factorizedlikelihood/single_psr_lookup_v4/'\n",
    "noisefile = '/home/nima/nanograv/11yr_burst_factorizedlikelihood/noisefiles/noisedict.json'\n",
    "\n",
    "psrname = 'J1713+0747'\n",
    "\n",
    "## Load the pulsar\n",
    "\n",
    "with open(pkl_path, 'rb') as f:\n",
    "    allpsrs=pickle.load(f)\n",
    "    \n",
    "for psr_i in allpsrs:\n",
    "    if psr_i.name == psrname:\n",
    "        psr = psr_i\n",
    "\n",
    "## Build grid spacing\n",
    "\n",
    "amp_spacing = '-20,-12,80'\n",
    "log10_amps = np.linspace(-20, -12, 80, endpoint=True)\n",
    "\n",
    "gamma_spacing ='0,7,70'\n",
    "gammas = np.linspace(0, 7, 70, endpoint=True)\n",
    "\n",
    "tmin = psr.toas.min() / const.day\n",
    "tmax = psr.toas.max() / const.day\n",
    "\n",
    "\n",
    "U,_ = utils.create_quantization_matrix(psr.toas)\n",
    "eps = 9  # clip first and last N observing epochs\n",
    "t0min = np.floor(max(U[:,eps] * psr.toas/const.day))\n",
    "t0max = np.ceil(max(U[:,-eps] * psr.toas/const.day))\n",
    "\n",
    "Ts = np.linspace(t0min, t0max, num=100, endpoint=True)\n",
    "time_spacing = '{},{},100'.format(t0min, t0max)\n",
    "sign_spacing = '-1,1,2'\n",
    "\n",
    "\n",
    "## Some bookkeeping\n",
    "\n",
    "if not os.path.exists(lookup_outdir + psr.name):\n",
    "    os.mkdir(lookup_outdir + psr.name)\n",
    "\n",
    "\n",
    "with open(lookup_outdir+'{}/pars.txt'.format(psr.name), 'w+') as f:\n",
    "        f.write('{}_red_noise_gamma;{}\\n{}_red_noise_log10_A;{}\\n{};{}\\n{};{}\\n{};{}'.format(psr.name,gamma_spacing,psr.name,amp_spacing, 'ramp_log10_A',amp_spacing, 'ramp_t0',time_spacing,'sign', sign_spacing))\n",
    "\n",
    "## Let it rip! We're doing the signs in parallel to speed things up, and we'll just add them back up\n",
    "## at the end.\n",
    "\n",
    "params1=[psr, noisefile, lookup_outdir, 1, log10_amps, amp_spacing, gammas, gamma_spacing, Ts, time_spacing]\n",
    "params2=[psr, noisefile, lookup_outdir, -1, log10_amps, amp_spacing, gammas, gamma_spacing, Ts, time_spacing]\n",
    "\n",
    "pool = mp.Pool(2)\n",
    "pool.starmap(make_lookup_table, [params1, params2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the two different signed lookuptables to make the full table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## just a quick concatenation of the two files\n",
    "## currently, they're produced in parallel so there's one for negative-signed, one is positive-signed\n",
    "neg_signs_file = lookup_outdir + '{}/{}_-1.txt'.format(psrname, psrname)\n",
    "pos_signs_file = lookup_outdir + '{}/{}_1.txt'.format(psrname, psrname)\n",
    "\n",
    "combined_table = lookup_outdir + '{}/{}_lookup.txt'.format(psrname, psrname)\n",
    "\n",
    "filenames = [neg_signs_file, pos_signs_file]\n",
    "with open(combined_table, 'w+') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at likelihood surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfile = os.path.join(lookup_outdir,psrname,'{:}_lookup.txt'.format(psrname))\n",
    "pfile = os.path.join(lookup_outdir,psrname,'pars.txt')\n",
    "\n",
    "lookup = np.loadtxt(lfile)\n",
    "params = np.loadtxt(pfile, dtype=str)\n",
    "\n",
    "Ngrid = [70, 60, 60, 100, 2]  # gamma, A_RN, A_BWM, t0, signs (This is the order in the likelihood table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the observing epochs for this psr\n",
    "U,_ = utils.create_quantization_matrix(psr.toas)\n",
    "eps = 9  # clip first and last N observing epochs\n",
    "t0min = np.floor(max(U[:,eps] * psr.toas/const.day))\n",
    "t0max = np.ceil(max(U[:,-eps] * psr.toas/const.day))\n",
    "\n",
    "RN_gams = np.linspace(0, 7, 70, endpoint=True)\n",
    "RN_amps = np.linspace(-20, -12, 80, endpoint=True)\n",
    "BWM_amps = np.linspace(-18, -12, 80, endpoint=True)\n",
    "BWM_t0s = np.linspace(t0min,t0max,100, endpoint=True)\n",
    "BWM_signs = np.linspace(-1,1,2, endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(70*80*80*100*2, len(lookup)) #These ought to be the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logL = lookup[:].reshape((2,100,80,80,70)) #reshape the loglikelihoods to get each column\n",
    "#this corresponds to [sign, burst_t0, burst_amp, rn_amp, rn_gamma]\n",
    "logL -= np.min(logL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get marginalized-ish likelihoods (the -ish is because we're simply summing over the other parameters to marginalize over them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logL_RNgam = np.sum(logL, axis=(0,1,2,3))\n",
    "logL_RNamp = np.sum(logL, axis=(0,1,2,4))\n",
    "logL_BWMamp = np.sum(logL, axis=(0,1,3,4))\n",
    "logL_BWMt0 = np.sum(logL, axis=(0,2,3,4))\n",
    "logL_BWMsign = np.sum(logL, axis=(1,2,3,4))\n",
    "\n",
    "#Just make sure they're the right shapes still\n",
    "len(logL_RNgam), len(logL_RNamp), len(logL_BWMamp), len(logL_BWMt0), len(logL_BWMsign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the marginalized posteriors to make sure they look reasonable. Note that the gamma posterior kind of looks goofy, not sure why, but if everything else looks ok, it's probably fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(12,6))\n",
    "ax[0,0].plot(RN_gams, logL_RNgam, label=r'$\\gamma$')\n",
    "ax[0,1].plot(RN_amps, logL_RNamp, label=r'$A_\\mathrm{RN}$')\n",
    "ax[1,0].plot(BWM_amps, logL_BWMamp, label=r'$A_\\mathrm{BWM}$')\n",
    "ax[1,1].plot(BWM_t0s, logL_BWMt0, label=r'$t_0$')\n",
    "ax[2,0].plot(BWM_signs, logL_BWMsign, label='$sign$')\n",
    "for a in ax.flatten():\n",
    "    a.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the 2-d marginalized A vs gamma to see if we're getting a reasonable red-noise detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"marginalize\" over burst params\n",
    "logL_RN = np.sum(logL, axis=(0,1,2))\n",
    "logL_RN.shape\n",
    "\n",
    "plt.contourf(RN_gams, RN_amps, logL_RN, cmap='Blues')\n",
    "plt.xlabel(r'$\\gamma$')\n",
    "plt.ylabel(r'$A_\\mathrm{RN}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the likelihood to do a single pulsar run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the pulsar we want and instantiate our LookupLikelihood pta. Presumably, the psrname and locations of all the files are the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lookupdir = lookup_outdir #Expect it to be in the place from above\n",
    "# For the lookupdirectory passed in, it should be the master-directory that contains the directory\n",
    "# for each pulsar. This is because in a multi-psr run, the pta needs to see every pulsar\n",
    "sampling_outdir = '/home/nima/nanograv/11yr_burst_factorizedlikelihood/sgl_psr_run/ramp_factorized_v4/{}/'.format(psrname)\n",
    "\n",
    "\n",
    "U,_ = utils.create_quantization_matrix(psr.toas)\n",
    "eps = 9  # clip first and last N observing epochs\n",
    "t0min = np.floor(max(U[:,eps] * psr.toas/const.day))\n",
    "t0max = np.ceil(max(U[:,-eps] * psr.toas/const.day))\n",
    "#Calculate the epochs for the pulsar\n",
    "\n",
    "with open(noisefile, 'rb') as f:\n",
    "    noisedict = json.load(f)\n",
    "\n",
    "pta = ee_models.model_ramp([psr], LookupLikelihood, lookupdir=lookupdir, upper_limit=False, bayesephem=False,\n",
    "                          Tmin_bwm=t0min, Tmax_bwm=t0max, noisedict=noisedict)\n",
    "\n",
    "\n",
    "\n",
    "# Save the pta parameters and prior distributions\n",
    "np.savetxt(sampling_outdir+'pars.txt',list(map(str, pta.param_names)), fmt='%s')\n",
    "np.savetxt(sampling_outdir+'priors.txt',list(map(lambda x: str(x.__repr__()), pta.params)), fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up sampler and begin sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_outdir = '/home/nima/nanograv/11yr_burst_factorizedlikelihood/sgl_psr_run/ramp_factorized_v4/{}'.format(psrname)\n",
    "x0 = np.hstack(p.sample() for p in pta.params)\n",
    "ndim = len(x0)\n",
    "\n",
    "# initial jump covariance matrix\n",
    "cov = np.diag(np.ones(ndim) * 0.1**2)\n",
    "sampler = ptmcmc(ndim, pta.get_lnlikelihood, pta.get_lnprior, cov,  outDir=sampling_outdir, resume=False)\n",
    "\n",
    "jp = ee_sampler.JumpProposal(pta)\n",
    "sampler.addProposalToCycle(jp.draw_from_prior, 30)\n",
    "\n",
    "sampler.sample(x0, int(3e5), SCAMweight=30, AMweight=50, DEweight=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking out the posteriors of the above runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la_forge.core import Core\n",
    "from la_forge.diagnostics import plot_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chaindir=sampling_outdir\n",
    "factorized_core = Core(label=psr.name + ' Factorized Run', chaindir=chaindir)\n",
    "\n",
    "plot_chains(factorized_core, hist=False, \n",
    "            suptitle='Factorized Likelihood Posterior Traces',\n",
    "            exclude=[\"lnlike\", \"chain_accept\", \"pt_chain_accept\", \"lnprior\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_chains(factorized_core, hist=True, \n",
    "            suptitle='Factorized Likelihood Posterior Traces',\n",
    "            exclude=[\"lnlike\", \"chain_accept\", \"pt_chain_accept\", \"lnprior\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
